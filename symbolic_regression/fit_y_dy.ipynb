{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b9c5fb1-92cb-4eb5-b489-ee4d1f661d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m git-repo `https://github.com/MilesCranmer/SymbolicRegression.jl.git`\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.8/Project.toml`\n",
      " \u001b[90m [8254be44] \u001b[39m\u001b[93m~ SymbolicRegression v0.22.4 ⇒ v0.22.2 `https://github.com/MilesCranmer/SymbolicRegression.jl.git#support-extra-data`\u001b[39m\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.8/Manifest.toml`\n",
      " \u001b[90m [8254be44] \u001b[39m\u001b[93m~ SymbolicRegression v0.22.4 ⇒ v0.22.2 `https://github.com/MilesCranmer/SymbolicRegression.jl.git#support-extra-data`\u001b[39m\n",
      "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n",
      "\u001b[32m  ✓ \u001b[39mSymbolicRegression\n",
      "  1 dependency successfully precompiled in 42 seconds. 266 already precompiled.\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.add(url = \"https://github.com/MilesCranmer/SymbolicRegression.jl.git\", rev = \"support-extra-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bd3c689-1c67-4bb2-a354-f539dbf47084",
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLJ  # for fit/predict\n",
    "using SymbolicRegression  # for SRRegressor\n",
    "using Zygote  # For `enable_autodiff=true`\n",
    "using SymbolicUtils\n",
    "using NPZ\n",
    "using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf556f48-a430-4f2d-9120-1e71287c057b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deriv_f (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_f(x) = x^3 / 3 - cos(x)\n",
    "deriv_f(x) = x^2 + sin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c38a5f92-cbf5-4d6f-b6bc-ab9bedef94da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32-element Vector{Float64}:\n",
       "  0.0\n",
       "  0.41696656061611775\n",
       "  1.006795441362392\n",
       "  1.7407915683009982\n",
       "  2.596415860289225\n",
       "  3.5595736030415055\n",
       "  4.626045473685325\n",
       "  5.801915925084421\n",
       "  7.102955436427127\n",
       "  8.55301934966111\n",
       " 10.181625856572422\n",
       " 12.020959041455523\n",
       " 14.102601257946091\n",
       "  ⋮\n",
       " 41.0765492048505\n",
       " 45.58145539714299\n",
       " 50.248209128707636\n",
       " 55.050052009553035\n",
       " 59.96730333407156\n",
       " 64.98935824662338\n",
       " 70.11576444366216\n",
       " 75.35626809573589\n",
       " 80.7298243269406\n",
       " 86.26267271702045\n",
       " 91.98567321877702\n",
       " 97.93117297286523"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = reshape(0.0:0.32:10.0, :, 1)\n",
    "y = true_f.(X[:, 1])\n",
    "∂y = deriv_f.(X[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fad36d59-57e3-4259-9aaf-65e11eb7774e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "derivative_loss (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function derivative_loss(tree, dataset::Dataset{T,L}, options, idx) where {T,L}\n",
    "    # Select from the batch indices, if given\n",
    "    X = idx === nothing ? dataset.X : view(dataset.X, :, idx)\n",
    "\n",
    "    # Evaluate both f(x) and f'(x), where f is defined by `tree`\n",
    "    ŷ, ∂ŷ, completed = eval_grad_tree_array(tree, X, options; variable=true)\n",
    "\n",
    "    #println(size(dataset.extra.∂y))\n",
    "\n",
    "    !completed && return L(Inf)\n",
    "\n",
    "    y = idx === nothing ? dataset.y : view(dataset.y, idx)\n",
    "    ∂y = idx === nothing ? dataset.extra.∂y : view(dataset.extra.∂y, idx)\n",
    "\n",
    "    mse_deriv = sum(i -> (∂ŷ[i] - ∂y[i])^2, eachindex(∂y)) / length(∂y)\n",
    "    mse_value = sum(i -> (ŷ[i] - y[i])^2, eachindex(y)) / length(y)\n",
    "\n",
    "    return mse_value  + mse_deriv\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2939d67f-5a44-4d2d-a2dc-71193d1806e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = SRRegressor(;\n",
    "    binary_operators=[+, -, *],\n",
    "    unary_operators=[cos],\n",
    "    loss_function=derivative_loss,\n",
    "    enable_autodiff=true,\n",
    "    batching=true,\n",
    "    batch_size=25,\n",
    "    niterations=100,\n",
    "    early_stop_condition=1e-6,\n",
    ")\n",
    "mach = machine(model, X, y, (; ∂y=∂y))\n",
    "fit!(mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8235ac-9c4d-4fc1-b478-c9492cc1d50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the eta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bb304dc-e02d-4da3-b569-90c701a2596e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 6), (1000,), (1000, 6))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in numpy data\n",
    "\n",
    "skip = 10\n",
    "\n",
    "jacobian = npzread(\"mean_jacobian_for_sr.npy\")\n",
    "\n",
    "X = npzread(\"theta_eta_for_sr_model_0.npy\")[1:skip:10000, 1:6]\n",
    "\n",
    "y = npzread(\"theta_eta_for_sr_model_0.npy\")[1:skip:10000, 7]\n",
    "\n",
    "\n",
    "∂y = jacobian[1:skip:10000, 1, :] # learn the first element of the jacobian row \n",
    "# this will be the integral wrt A\n",
    "\n",
    "size(X), size(y), size(∂y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44175913-eb6f-45f6-abba-afffc2b2eaa9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mThe number and/or types of data arguments do not match what the specified model\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39msupports. Suppress this type check by specifying `scitype_check_level=0`.\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mRun `@doc SymbolicRegression.SRRegressor` to learn more about your model's requirements.\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mCommonly, but non exclusively, supervised models are constructed using the syntax\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m`machine(model, X, y)` or `machine(model, X, y, w)` while most other models are\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mconstructed with `machine(model, X)`.  Here `X` are features, `y` a target, and `w`\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39msample or class weights.\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mIn general, data in `machine(model, data...)` is expected to satisfy\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m    scitype(data) <: MLJ.fit_data_scitype(model)\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mIn the present case:\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mscitype(data) = Tuple{AbstractMatrix{Continuous}, AbstractVector{Continuous}, Unknown}\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mfit_data_scitype(model) = Union{Tuple{Union{Table{<:AbstractVector{<:Continuous}}, AbstractMatrix{<:Continuous}}, AbstractVector{<:Continuous}}, Tuple{Union{Table{<:AbstractVector{<:Continuous}}, AbstractMatrix{<:Continuous}}, AbstractVector{<:Continuous}, AbstractVector{<:Union{Continuous, Count}}}}\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ MLJBase ~/.julia/packages/MLJBase/ByFwA/src/machines.jl:230\u001b[39m\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(SRRegressor(binary_operators = Function[+, *, ^], …), …).\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mYou are using an experimental interface for the `extra` field of a `Dataset` type. This API may change in the future.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ SymbolicRegression.MLJInterfaceModule ~/.julia/packages/SymbolicRegression/fHd3u/src/MLJInterface.jl:208\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mYou are using multithreading mode, but only one thread is available. Try starting julia with `--threads=auto`.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ SymbolicRegression ~/.julia/packages/SymbolicRegression/fHd3u/src/SymbolicRegression.jl:553\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0%┣                                           ┫ 0/1.5k [00:01<-18:-57, -1s/it]Expressions evaluated per second: [.....]. Head worker occupation: 0.0%         Press 'q' and then <enter> to stop execution early.                             Hall of Fame:                                                                   ---------------------------------------------------------------------------------------------------                                                             Complexity  Loss       Score     Equation                                       1           1.600e+04  1.594e+01  y = x₁                                        3           5.881e+03  5.004e-01  y = (-12.901 * -8.3245)                       ---------------------------------------------------------------------------------------------------\n",
      "0.1%┣                                           ┫ 2/1.5k [00:03<01:03:09, 3s/it]Expressions evaluated per second: 4.29e+02. Head worker occupation: 0.0%        Press 'q' and then <enter> to stop execution early.                             Hall of Fame:                                                                   ---------------------------------------------------------------------------------------------------                                                             Complexity  Loss       Score     Equation                                       1           1.598e+04  1.594e+01  y = x₃                                        3           5.388e+03  5.437e-01  y = (x₃ * 68.495)                             ---------------------------------------------------------------------------------------------------\n",
      "0.3%┣▏                                             ┫ 4/1.5k [00:07<56:13, 2s/it]Expressions evaluated per second: 3.33e+02. Head worker occupation: 0.0%        Press 'q' and then <enter> to stop execution early.                             Hall of Fame:                                                                   ---------------------------------------------------------------------------------------------------                                                             Complexity  Loss       Score     Equation                                       1           1.598e+04  1.594e+01  y = x₃                                        3           5.388e+03  5.437e-01  y = (x₃ * 68.495)                             ---------------------------------------------------------------------------------------------------\n",
      "0.4%┣▏                                             ┫ 6/1.5k [00:10<51:08, 2s/it]Expressions evaluated per second: 3.28e+02. Head worker occupation: 0.0%        Press 'q' and then <enter> to stop execution early.                             Hall of Fame:                                                                   ---------------------------------------------------------------------------------------------------                                                             Complexity  Loss       Score     Equation                                       1           1.598e+04  1.594e+01  y = x₃                                        3           5.388e+03  5.437e-01  y = (x₃ * 68.495)                             ---------------------------------------------------------------------------------------------------\n",
      "0.5%┣▎                                             ┫ 8/1.5k [00:14<51:13, 2s/it]Expressions evaluated per second: 3.11e+02. Head worker occupation: 0.0%        Press 'q' and then <enter> to stop execution early.                             Hall of Fame:                                                                   ---------------------------------------------------------------------------------------------------                                                             Complexity  Loss       Score     Equation                                       1           1.598e+04  1.594e+01  y = x₃                                        3           5.388e+03  5.437e-01  y = (x₃ * 68.495)                             ---------------------------------------------------------------------------------------------------\n",
      "0.7%┣▎                                            ┫ 10/1.5k [00:19<52:13, 2s/it]Expressions evaluated per second: 2.99e+02. Head worker occupation: 0.0%        Press 'q' and then <enter> to stop execution early.                             Hall of Fame:                                                                   ---------------------------------------------------------------------------------------------------                                                             Complexity  Loss       Score     Equation                                       1           5.857e+03  1.594e+01  y = 100.81                                    3           5.388e+03  4.175e-02  y = (x₃ * 68.495)                             ---------------------------------------------------------------------------------------------------\n",
      "0.8%┣▍                                            ┫ 12/1.5k [00:22<49:34, 2s/it]Expressions evaluated per second: 3.13e+02. Head worker occupation: 0.0%        Press 'q' and then <enter> to stop execution early.                             Hall of Fame:                                                                   ---------------------------------------------------------------------------------------------------                                                             Complexity  Loss       Score     Equation                                       1           5.857e+03  1.594e+01  y = 100.81                                    3           5.388e+03  4.175e-02  y = (x₃ * 68.495)                             ---------------------------------------------------------------------------------------------------\n",
      "0.9%┣▍                                            ┫ 14/1.5k [00:25<48:32, 2s/it]Expressions evaluated per second: 3.02e+02. Head worker occupation: 0.0%        Press 'q' and then <enter> to stop execution early.                             Hall of Fame:                                                                   ---------------------------------------------------------------------------------------------------                                                             Complexity  Loss       Score     Equation                                       1           5.857e+03  1.594e+01  y = 100.81                                    3           5.388e+03  4.175e-02  y = (x₃ * 68.495)                             ---------------------------------------------------------------------------------------------------\n",
      "\u001b[1A\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mTerminated early due to NaN in gradient.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Optim ~/.julia/packages/Optim/Adqv3/src/multivariate/optimize/optimize.jl:98\u001b[39m\n",
      "\u001b[1A\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mTerminated early due to NaN in gradient.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Optim ~/.julia/packages/Optim/Adqv3/src/multivariate/optimize/optimize.jl:98\u001b[39m\n",
      "\u001b[1A\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mTerminated early due to NaN in gradient.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Optim ~/.julia/packages/Optim/Adqv3/src/multivariate/optimize/optimize.jl:98\u001b[39m\n",
      "1.1%┣▌                                            ┫ 16/1.5k [00:30<49:03, 2s/it]Expressions evaluated per second: 3.00e+02. Head worker occupation: 0.0%        Press 'q' and then <enter> to stop execution early.                             Hall of Fame:                                                                   ---------------------------------------------------------------------------------------------------                                                             Complexity  Loss       Score     Equation                                       1           5.857e+03  1.594e+01  y = 100.81                                    3           5.388e+03  4.175e-02  y = (x₃ * 68.495)                             ---------------------------------------------------------------------------------------------------\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mTerminated early due to NaN in gradient.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Optim ~/.julia/packages/Optim/Adqv3/src/multivariate/optimize/optimize.jl:98\u001b[39m\n",
      "1.2%┣▌                                            ┫ 18/1.5k [00:32<46:07, 2s/it]Expressions evaluated per second: 3.40e+02. Head worker occupation: 0.0%        Press 'q' and then <enter> to stop execution early.                             Hall of Fame:                                                                   ---------------------------------------------------------------------------------------------------                                                             Complexity  Loss       Score     Equation                                       1           5.857e+03  1.594e+01  y = 100.81                                    3           5.388e+03  4.175e-02  y = (x₃ * 68.495)                             ---------------------------------------------------------------------------------------------------\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mTerminated early due to NaN in gradient.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Optim ~/.julia/packages/Optim/Adqv3/src/multivariate/optimize/optimize.jl:98\u001b[39m\n",
      "\u001b[1A\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mTerminated early due to NaN in gradient.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Optim ~/.julia/packages/Optim/Adqv3/src/multivariate/optimize/optimize.jl:98\u001b[39m\n",
      "1.3%┣▋                                            ┫ 20/1.5k [00:37<47:24, 2s/it]Expressions evaluated per second: 3.63e+02. Head worker occupation: 0.0%        Press 'q' and then <enter> to stop execution early.                             Hall of Fame:                                                                   ---------------------------------------------------------------------------------------------------                                                             Complexity  Loss       Score     Equation                                       1           5.857e+03  1.594e+01  y = 100.81                                    3           5.388e+03  4.175e-02  y = (x₃ * 68.495)                             ---------------------------------------------------------------------------------------------------\n",
      "1.5%┣▋                                            ┫ 22/1.5k [00:44<51:37, 2s/it]Expressions evaluated per second: 3.47e+02. Head worker occupation: 0.0%        Press 'q' and then <enter> to stop execution early.                             Hall of Fame:                                                                   ---------------------------------------------------------------------------------------------------                                                             Complexity  Loss       Score     Equation                                       1           5.857e+03  1.594e+01  y = 100.81                                    3           5.388e+03  4.175e-02  y = (x₃ * 68.495)                             ---------------------------------------------------------------------------------------------------\n",
      "1.6%┣▊                                            ┫ 24/1.5k [00:49<51:56, 2s/it]Expressions evaluated per second: 3.35e+02. Head worker occupation: 0.1%        Press 'q' and then <enter> to stop execution early.                             Hall of Fame:                                                                   ---------------------------------------------------------------------------------------------------                                                             Complexity  Loss       Score     Equation                                       1           5.857e+03  1.594e+01  y = 100.81                                    3           5.388e+03  4.175e-02  y = (x₃ * 68.495)                             ---------------------------------------------------------------------------------------------------\n",
      "\u001b[91m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[91m\u001b[1mError: \u001b[22m\u001b[39mProblem fitting the machine machine(SRRegressor(binary_operators = Function[+, *, ^], …), …). \n",
      "\u001b[91m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ MLJBase ~/.julia/packages/MLJBase/ByFwA/src/machines.jl:682\u001b[39m\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mRunning type checks... \n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mThe number and/or types of data arguments do not match what the specified model\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39msupports. Suppress this type check by specifying `scitype_check_level=0`.\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mRun `@doc SymbolicRegression.SRRegressor` to learn more about your model's requirements.\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mCommonly, but non exclusively, supervised models are constructed using the syntax\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m`machine(model, X, y)` or `machine(model, X, y, w)` while most other models are\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mconstructed with `machine(model, X)`.  Here `X` are features, `y` a target, and `w`\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39msample or class weights.\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mIn general, data in `machine(model, data...)` is expected to satisfy\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m    scitype(data) <: MLJ.fit_data_scitype(model)\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mIn the present case:\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mscitype(data) = Tuple{AbstractMatrix{Continuous}, AbstractVector{Continuous}, Unknown}\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mfit_data_scitype(model) = Union{Tuple{Union{Table{<:AbstractVector{<:Continuous}}, AbstractMatrix{<:Continuous}}, AbstractVector{<:Continuous}}, Tuple{Union{Table{<:AbstractVector{<:Continuous}}, AbstractMatrix{<:Continuous}}, AbstractVector{<:Continuous}, AbstractVector{<:Union{Continuous, Count}}}}\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ MLJBase ~/.julia/packages/MLJBase/ByFwA/src/machines.jl:230\u001b[39m\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mIt seems an upstream node in a learning network is providing data of incompatible scitype. See above. \n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "TaskFailedException\n\n\u001b[91m    nested task error: \u001b[39mTaskFailedException\n    Stacktrace:\n     [1] \u001b[0m\u001b[1mwait\u001b[22m\n    \u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mtask.jl:345\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n     [2] \u001b[0m\u001b[1mfetch\u001b[22m\n    \u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mtask.jl:360\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n     [3] \u001b[0m\u001b[1m(::SymbolicRegression.var\"#53#80\"{Vector{Vector{Channel{Any}}}, Vector{Vector{Task}}, Int64, Int64})\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[0m\u001b[1m)\u001b[22m\n    \u001b[90m   @ \u001b[39m\u001b[35mSymbolicRegression\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mtask.jl:484\u001b[24m\u001b[39m\n    \n    \u001b[91m    nested task error: \u001b[39mInterruptException:\n        Stacktrace:\n          [1] \u001b[0m\u001b[1mPullback\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/SymbolicRegression/fHd3u/src/\u001b[39m\u001b[90m\u001b[4mOperators.jl:47\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n          [2] \u001b[0m\u001b[1m(::Zygote.Pullback{Tuple{typeof(safe_pow), Float32, Float32}, Any})\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mΔ\u001b[39m::\u001b[0mFloat32\u001b[0m\u001b[1m)\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[35mZygote\u001b[39m \u001b[90m~/.julia/packages/Zygote/4rucm/src/compiler/\u001b[39m\u001b[90m\u001b[4minterface2.jl:0\u001b[24m\u001b[39m\n          [3] \u001b[0m\u001b[1m(::Zygote.var\"#75#76\"{Zygote.Pullback{Tuple{typeof(safe_pow), Float32, Float32}, Any}})\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mΔ\u001b[39m::\u001b[0mFloat32\u001b[0m\u001b[1m)\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[35mZygote\u001b[39m \u001b[90m~/.julia/packages/Zygote/4rucm/src/compiler/\u001b[39m\u001b[90m\u001b[4minterface.jl:45\u001b[24m\u001b[39m\n          [4] \u001b[0m\u001b[1mgradient\u001b[22m\u001b[0m\u001b[1m(\u001b[22m::\u001b[0mFunction, ::\u001b[0mFloat32, ::\u001b[0mVararg\u001b[90m{Float32}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[35mZygote\u001b[39m \u001b[90m~/.julia/packages/Zygote/4rucm/src/compiler/\u001b[39m\u001b[90m\u001b[4minterface.jl:97\u001b[24m\u001b[39m\n          [5] \u001b[0m\u001b[1m#3\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/DynamicExpressions/KRT17/ext_compat/\u001b[39m\u001b[90m\u001b[4mDynamicExpressionsZygoteExt.jl:14\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n          [6] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/DynamicExpressions/KRT17/src/\u001b[39m\u001b[90m\u001b[4mEvaluateEquationDerivative.jl:373\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n          [7] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4msimdloop.jl:77\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n          [8] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/DynamicExpressions/KRT17/src/\u001b[39m\u001b[90m\u001b[4mUtils.jl:54\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n          [9] \u001b[0m\u001b[1mgrad_deg2_eval\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mtree\u001b[39m::\u001b[0mDynamicExpressions.EquationModule.Node\u001b[90m{Float32}\u001b[39m, \u001b[90m#unused#\u001b[39m::\u001b[0mVal\u001b[90m{6}\u001b[39m, \u001b[90mindex_tree\u001b[39m::\u001b[0mDynamicExpressions.EquationUtilsModule.NodeIndex, \u001b[90mcX\u001b[39m::\u001b[0mSubArray\u001b[90m{Float32, 2, Matrix{Float32}, Tuple{Base.Slice{Base.OneTo{Int64}}, Vector{Int64}}, false}\u001b[39m, \u001b[90mop\u001b[39m::\u001b[0mtypeof(safe_pow), \u001b[90mdiff_op\u001b[39m::\u001b[0mDynamicExpressions.DynamicExpressionsZygoteExt.var\"#3#4\"\u001b[90m{typeof(safe_pow)}\u001b[39m, \u001b[90moperators\u001b[39m::\u001b[0mDynamicExpressions.OperatorEnumModule.OperatorEnum, \u001b[90m#unused#\u001b[39m::\u001b[0mVal\u001b[90m{true}\u001b[39m, \u001b[90m#unused#\u001b[39m::\u001b[0mVal\u001b[90m{false}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[36mDynamicExpressions.EvaluateEquationDerivativeModule\u001b[39m \u001b[90m~/.julia/packages/DynamicExpressions/KRT17/src/\u001b[39m\u001b[90m\u001b[4mEvaluateEquationDerivative.jl:367\u001b[24m\u001b[39m\n         [10] \u001b[0m\u001b[1m_eval_grad_tree_array\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mtree\u001b[39m::\u001b[0mDynamicExpressions.EquationModule.Node\u001b[90m{Float32}\u001b[39m, \u001b[90m#unused#\u001b[39m::\u001b[0mVal\u001b[90m{6}\u001b[39m, \u001b[90mindex_tree\u001b[39m::\u001b[0mDynamicExpressions.EquationUtilsModule.NodeIndex, \u001b[90mcX\u001b[39m::\u001b[0mSubArray\u001b[90m{Float32, 2, Matrix{Float32}, Tuple{Base.Slice{Base.OneTo{Int64}}, Vector{Int64}}, false}\u001b[39m, \u001b[90moperators\u001b[39m::\u001b[0mDynamicExpressions.OperatorEnumModule.OperatorEnum, \u001b[90m#unused#\u001b[39m::\u001b[0mVal\u001b[90m{true}\u001b[39m, \u001b[90m#unused#\u001b[39m::\u001b[0mVal\u001b[90m{false}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[36mDynamicExpressions.EvaluateEquationDerivativeModule\u001b[39m \u001b[90m~/.julia/packages/DynamicExpressions/KRT17/src/\u001b[39m\u001b[90m\u001b[4mEvaluateEquationDerivative.jl:276\u001b[24m\u001b[39m\n         [11] \u001b[0m\u001b[1meval_grad_tree_array\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/DynamicExpressions/KRT17/src/\u001b[39m\u001b[90m\u001b[4mEvaluateEquationDerivative.jl:226\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n         [12] \u001b[0m\u001b[1mgrad_deg2_eval\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mtree\u001b[39m::\u001b[0mDynamicExpressions.EquationModule.Node\u001b[90m{Float32}\u001b[39m, \u001b[90m#unused#\u001b[39m::\u001b[0mVal\u001b[90m{6}\u001b[39m, \u001b[90mindex_tree\u001b[39m::\u001b[0mDynamicExpressions.EquationUtilsModule.NodeIndex, \u001b[90mcX\u001b[39m::\u001b[0mSubArray\u001b[90m{Float32, 2, Matrix{Float32}, Tuple{Base.Slice{Base.OneTo{Int64}}, Vector{Int64}}, false}\u001b[39m, \u001b[90mop\u001b[39m::\u001b[0mtypeof(+), \u001b[90mdiff_op\u001b[39m::\u001b[0mDynamicExpressions.DynamicExpressionsZygoteExt.var\"#3#4\"\u001b[90m{typeof(+)}\u001b[39m, \u001b[90moperators\u001b[39m::\u001b[0mDynamicExpressions.OperatorEnumModule.OperatorEnum, \u001b[90m#unused#\u001b[39m::\u001b[0mVal\u001b[90m{true}\u001b[39m, \u001b[90m#unused#\u001b[39m::\u001b[0mVal\u001b[90m{false}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[36mDynamicExpressions.EvaluateEquationDerivativeModule\u001b[39m \u001b[90m~/.julia/packages/DynamicExpressions/KRT17/src/\u001b[39m\u001b[90m\u001b[4mEvaluateEquationDerivative.jl:358\u001b[24m\u001b[39m\n         [13] \u001b[0m\u001b[1m_eval_grad_tree_array\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mtree\u001b[39m::\u001b[0mDynamicExpressions.EquationModule.Node\u001b[90m{Float32}\u001b[39m, \u001b[90m#unused#\u001b[39m::\u001b[0mVal\u001b[90m{6}\u001b[39m, \u001b[90mindex_tree\u001b[39m::\u001b[0mDynamicExpressions.EquationUtilsModule.NodeIndex, \u001b[90mcX\u001b[39m::\u001b[0mSubArray\u001b[90m{Float32, 2, Matrix{Float32}, Tuple{Base.Slice{Base.OneTo{Int64}}, Vector{Int64}}, false}\u001b[39m, \u001b[90moperators\u001b[39m::\u001b[0mDynamicExpressions.OperatorEnumModule.OperatorEnum, \u001b[90m#unused#\u001b[39m::\u001b[0mVal\u001b[90m{true}\u001b[39m, \u001b[90m#unused#\u001b[39m::\u001b[0mVal\u001b[90m{false}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[36mDynamicExpressions.EvaluateEquationDerivativeModule\u001b[39m \u001b[90m~/.julia/packages/DynamicExpressions/KRT17/src/\u001b[39m\u001b[90m\u001b[4mEvaluateEquationDerivative.jl:276\u001b[24m\u001b[39m\n         [14] \u001b[0m\u001b[1meval_grad_tree_array\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mtree\u001b[39m::\u001b[0mDynamicExpressions.EquationModule.Node\u001b[90m{Float32}\u001b[39m, \u001b[90m#unused#\u001b[39m::\u001b[0mVal\u001b[90m{6}\u001b[39m, \u001b[90mindex_tree\u001b[39m::\u001b[0mDynamicExpressions.EquationUtilsModule.NodeIndex, \u001b[90mcX\u001b[39m::\u001b[0mSubArray\u001b[90m{Float32, 2, Matrix{Float32}, Tuple{Base.Slice{Base.OneTo{Int64}}, Vector{Int64}}, false}\u001b[39m, \u001b[90moperators\u001b[39m::\u001b[0mDynamicExpressions.OperatorEnumModule.OperatorEnum, \u001b[90m#unused#\u001b[39m::\u001b[0mVal\u001b[90m{true}\u001b[39m, \u001b[90m#unused#\u001b[39m::\u001b[0mVal\u001b[90m{false}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[36mDynamicExpressions.EvaluateEquationDerivativeModule\u001b[39m \u001b[90m~/.julia/packages/DynamicExpressions/KRT17/src/\u001b[39m\u001b[90m\u001b[4mEvaluateEquationDerivative.jl:226\u001b[24m\u001b[39m\n         [15] \u001b[0m\u001b[1meval_grad_tree_array\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mtree\u001b[39m::\u001b[0mDynamicExpressions.EquationModule.Node\u001b[90m{Float32}\u001b[39m, \u001b[90mcX\u001b[39m::\u001b[0mSubArray\u001b[90m{Float32, 2, Matrix{Float32}, Tuple{Base.Slice{Base.OneTo{Int64}}, Vector{Int64}}, false}\u001b[39m, \u001b[90moperators\u001b[39m::\u001b[0mDynamicExpressions.OperatorEnumModule.OperatorEnum; \u001b[90mvariable\u001b[39m::\u001b[0mBool, \u001b[90mturbo\u001b[39m::\u001b[0mBool\u001b[0m\u001b[1m)\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[36mDynamicExpressions.EvaluateEquationDerivativeModule\u001b[39m \u001b[90m~/.julia/packages/DynamicExpressions/KRT17/src/\u001b[39m\u001b[90m\u001b[4mEvaluateEquationDerivative.jl:204\u001b[24m\u001b[39m\n         [16] \u001b[0m\u001b[1m#eval_grad_tree_array#2\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/SymbolicRegression/fHd3u/src/\u001b[39m\u001b[90m\u001b[4mInterfaceDynamicExpressions.jl:112\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n         [17] \u001b[0m\u001b[1mderivative_loss\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mtree\u001b[39m::\u001b[0mDynamicExpressions.EquationModule.Node\u001b[90m{Float32}\u001b[39m, \u001b[90mdataset\u001b[39m::\u001b[0mDataset\u001b[90m{Float32, Float32, Matrix{Float32}, Vector{Float32}, Nothing, NamedTuple{(:∂y,), Tuple{Matrix{Float32}}}, Nothing, Nothing, Nothing, Nothing}\u001b[39m, \u001b[90moptions\u001b[39m::\u001b[0mOptions\u001b[90m{Int64, DynamicExpressions.OperatorEnumModule.OperatorEnum, false, Optim.Options{Float64, Nothing}, StatsBase.Weights{Float64, Float64, Vector{Float64}}}\u001b[39m, \u001b[90midx\u001b[39m::\u001b[0mVector\u001b[90m{Int64}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[32mMain\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mIn[5]:6\u001b[24m\u001b[39m\n         [18] \u001b[0m\u001b[1mevaluator\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mf\u001b[39m::\u001b[0mtypeof(derivative_loss), \u001b[90mtree\u001b[39m::\u001b[0mDynamicExpressions.EquationModule.Node\u001b[90m{Float32}\u001b[39m, \u001b[90mdataset\u001b[39m::\u001b[0mDataset\u001b[90m{Float32, Float32, Matrix{Float32}, Vector{Float32}, Nothing, NamedTuple{(:∂y,), Tuple{Matrix{Float32}}}, Nothing, Nothing, Nothing, Nothing}\u001b[39m, \u001b[90moptions\u001b[39m::\u001b[0mOptions\u001b[90m{Int64, DynamicExpressions.OperatorEnumModule.OperatorEnum, false, Optim.Options{Float64, Nothing}, StatsBase.Weights{Float64, Float64, Vector{Float64}}}\u001b[39m, \u001b[90midx\u001b[39m::\u001b[0mVector\u001b[90m{Int64}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[33mSymbolicRegression.LossFunctionsModule\u001b[39m \u001b[90m~/.julia/packages/SymbolicRegression/fHd3u/src/\u001b[39m\u001b[90m\u001b[4mLossFunctions.jl:80\u001b[24m\u001b[39m\n         [19] \u001b[0m\u001b[1meval_loss\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mtree\u001b[39m::\u001b[0mDynamicExpressions.EquationModule.Node\u001b[90m{Float32}\u001b[39m, \u001b[90mdataset\u001b[39m::\u001b[0mDataset\u001b[90m{Float32, Float32, Matrix{Float32}, Vector{Float32}, Nothing, NamedTuple{(:∂y,), Tuple{Matrix{Float32}}}, Nothing, Nothing, Nothing, Nothing}\u001b[39m, \u001b[90moptions\u001b[39m::\u001b[0mOptions\u001b[90m{Int64, DynamicExpressions.OperatorEnumModule.OperatorEnum, false, Optim.Options{Float64, Nothing}, StatsBase.Weights{Float64, Float64, Vector{Float64}}}\u001b[39m; \u001b[90mregularization\u001b[39m::\u001b[0mBool, \u001b[90midx\u001b[39m::\u001b[0mVector\u001b[90m{Int64}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[33mSymbolicRegression.LossFunctionsModule\u001b[39m \u001b[90m~/.julia/packages/SymbolicRegression/fHd3u/src/\u001b[39m\u001b[90m\u001b[4mLossFunctions.jl:105\u001b[24m\u001b[39m\n         [20] \u001b[0m\u001b[1m#eval_loss_batched#4\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/SymbolicRegression/fHd3u/src/\u001b[39m\u001b[90m\u001b[4mLossFunctions.jl:119\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n         [21] \u001b[0m\u001b[1m#score_func_batched#6\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/SymbolicRegression/fHd3u/src/\u001b[39m\u001b[90m\u001b[4mLossFunctions.jl:181\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n         [22] \u001b[0m\u001b[1ms_r_cycle\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mdataset\u001b[39m::\u001b[0mDataset\u001b[90m{Float32, Float32, Matrix{Float32}, Vector{Float32}, Nothing, NamedTuple{(:∂y,), Tuple{Matrix{Float32}}}, Nothing, Nothing, Nothing, Nothing}\u001b[39m, \u001b[90mpop\u001b[39m::\u001b[0mPopulation\u001b[90m{Float32, Float32}\u001b[39m, \u001b[90mncycles\u001b[39m::\u001b[0mInt64, \u001b[90mcurmaxsize\u001b[39m::\u001b[0mInt64, \u001b[90mrunning_search_statistics\u001b[39m::\u001b[0mSymbolicRegression.AdaptiveParsimonyModule.RunningSearchStatistics; \u001b[90mverbosity\u001b[39m::\u001b[0mInt64, \u001b[90moptions\u001b[39m::\u001b[0mOptions\u001b[90m{Int64, DynamicExpressions.OperatorEnumModule.OperatorEnum, false, Optim.Options{Float64, Nothing}, StatsBase.Weights{Float64, Float64, Vector{Float64}}}\u001b[39m, \u001b[90mrecord\u001b[39m::\u001b[0mDict\u001b[90m{String, Any}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[33mSymbolicRegression.SingleIterationModule\u001b[39m \u001b[90m~/.julia/packages/SymbolicRegression/fHd3u/src/\u001b[39m\u001b[90m\u001b[4mSingleIteration.jl:62\u001b[24m\u001b[39m\n         [23] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/SymbolicRegression/fHd3u/src/\u001b[39m\u001b[90m\u001b[4mSymbolicRegression.jl:955\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n         [24] \u001b[0m\u001b[1m(::SymbolicRegression.var\"#52#79\"{Options{Int64, DynamicExpressions.OperatorEnumModule.OperatorEnum, false, Optim.Options{Float64, Nothing}, StatsBase.Weights{Float64, Float64, Vector{Float64}}}, Int64, Population{Float32, Float32}, SymbolicRegression.AdaptiveParsimonyModule.RunningSearchStatistics, Int64, Dataset{Float32, Float32, Matrix{Float32}, Vector{Float32}, Nothing, NamedTuple{(:∂y,), Tuple{Matrix{Float32}}}, Nothing, Nothing, Nothing, Nothing}})\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[0m\u001b[1m)\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[33mSymbolicRegression\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mthreadingconstructs.jl:258\u001b[24m\u001b[39m",
     "output_type": "error",
     "traceback": [
      "TaskFailedException\n\n\u001b[91m    nested task error: \u001b[39mTaskFailedException\n    Stacktrace:\n     [1] \u001b[0m\u001b[1mwait\u001b[22m\n    \u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mtask.jl:345\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n     [2] \u001b[0m\u001b[1mfetch\u001b[22m\n    \u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mtask.jl:360\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n     [3] \u001b[0m\u001b[1m(::SymbolicRegression.var\"#53#80\"{Vector{Vector{Channel{Any}}}, Vector{Vector{Task}}, Int64, Int64})\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[0m\u001b[1m)\u001b[22m\n    \u001b[90m   @ \u001b[39m\u001b[35mSymbolicRegression\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mtask.jl:484\u001b[24m\u001b[39m\n    \n    \u001b[91m    nested task error: \u001b[39mInterruptException:\n        Stacktrace:\n          [1] \u001b[0m\u001b[1mPullback\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/SymbolicRegression/fHd3u/src/\u001b[39m\u001b[90m\u001b[4mOperators.jl:47\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n          [2] \u001b[0m\u001b[1m(::Zygote.Pullback{Tuple{typeof(safe_pow), Float32, Float32}, Any})\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mΔ\u001b[39m::\u001b[0mFloat32\u001b[0m\u001b[1m)\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[35mZygote\u001b[39m \u001b[90m~/.julia/packages/Zygote/4rucm/src/compiler/\u001b[39m\u001b[90m\u001b[4minterface2.jl:0\u001b[24m\u001b[39m\n          [3] \u001b[0m\u001b[1m(::Zygote.var\"#75#76\"{Zygote.Pullback{Tuple{typeof(safe_pow), Float32, Float32}, Any}})\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mΔ\u001b[39m::\u001b[0mFloat32\u001b[0m\u001b[1m)\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[35mZygote\u001b[39m \u001b[90m~/.julia/packages/Zygote/4rucm/src/compiler/\u001b[39m\u001b[90m\u001b[4minterface.jl:45\u001b[24m\u001b[39m\n          [4] \u001b[0m\u001b[1mgradient\u001b[22m\u001b[0m\u001b[1m(\u001b[22m::\u001b[0mFunction, ::\u001b[0mFloat32, ::\u001b[0mVararg\u001b[90m{Float32}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[35mZygote\u001b[39m \u001b[90m~/.julia/packages/Zygote/4rucm/src/compiler/\u001b[39m\u001b[90m\u001b[4minterface.jl:97\u001b[24m\u001b[39m\n          [5] \u001b[0m\u001b[1m#3\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/DynamicExpressions/KRT17/ext_compat/\u001b[39m\u001b[90m\u001b[4mDynamicExpressionsZygoteExt.jl:14\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n          [6] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/DynamicExpressions/KRT17/src/\u001b[39m\u001b[90m\u001b[4mEvaluateEquationDerivative.jl:373\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n          [7] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4msimdloop.jl:77\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n          [8] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/DynamicExpressions/KRT17/src/\u001b[39m\u001b[90m\u001b[4mUtils.jl:54\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n          [9] \u001b[0m\u001b[1mgrad_deg2_eval\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mtree\u001b[39m::\u001b[0mDynamicExpressions.EquationModule.Node\u001b[90m{Float32}\u001b[39m, \u001b[90m#unused#\u001b[39m::\u001b[0mVal\u001b[90m{6}\u001b[39m, \u001b[90mindex_tree\u001b[39m::\u001b[0mDynamicExpressions.EquationUtilsModule.NodeIndex, \u001b[90mcX\u001b[39m::\u001b[0mSubArray\u001b[90m{Float32, 2, Matrix{Float32}, Tuple{Base.Slice{Base.OneTo{Int64}}, Vector{Int64}}, false}\u001b[39m, \u001b[90mop\u001b[39m::\u001b[0mtypeof(safe_pow), \u001b[90mdiff_op\u001b[39m::\u001b[0mDynamicExpressions.DynamicExpressionsZygoteExt.var\"#3#4\"\u001b[90m{typeof(safe_pow)}\u001b[39m, \u001b[90moperators\u001b[39m::\u001b[0mDynamicExpressions.OperatorEnumModule.OperatorEnum, \u001b[90m#unused#\u001b[39m::\u001b[0mVal\u001b[90m{true}\u001b[39m, \u001b[90m#unused#\u001b[39m::\u001b[0mVal\u001b[90m{false}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[36mDynamicExpressions.EvaluateEquationDerivativeModule\u001b[39m \u001b[90m~/.julia/packages/DynamicExpressions/KRT17/src/\u001b[39m\u001b[90m\u001b[4mEvaluateEquationDerivative.jl:367\u001b[24m\u001b[39m\n         [10] \u001b[0m\u001b[1m_eval_grad_tree_array\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mtree\u001b[39m::\u001b[0mDynamicExpressions.EquationModule.Node\u001b[90m{Float32}\u001b[39m, \u001b[90m#unused#\u001b[39m::\u001b[0mVal\u001b[90m{6}\u001b[39m, \u001b[90mindex_tree\u001b[39m::\u001b[0mDynamicExpressions.EquationUtilsModule.NodeIndex, \u001b[90mcX\u001b[39m::\u001b[0mSubArray\u001b[90m{Float32, 2, Matrix{Float32}, Tuple{Base.Slice{Base.OneTo{Int64}}, Vector{Int64}}, false}\u001b[39m, \u001b[90moperators\u001b[39m::\u001b[0mDynamicExpressions.OperatorEnumModule.OperatorEnum, \u001b[90m#unused#\u001b[39m::\u001b[0mVal\u001b[90m{true}\u001b[39m, \u001b[90m#unused#\u001b[39m::\u001b[0mVal\u001b[90m{false}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[36mDynamicExpressions.EvaluateEquationDerivativeModule\u001b[39m \u001b[90m~/.julia/packages/DynamicExpressions/KRT17/src/\u001b[39m\u001b[90m\u001b[4mEvaluateEquationDerivative.jl:276\u001b[24m\u001b[39m\n         [11] \u001b[0m\u001b[1meval_grad_tree_array\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/DynamicExpressions/KRT17/src/\u001b[39m\u001b[90m\u001b[4mEvaluateEquationDerivative.jl:226\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n         [12] \u001b[0m\u001b[1mgrad_deg2_eval\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mtree\u001b[39m::\u001b[0mDynamicExpressions.EquationModule.Node\u001b[90m{Float32}\u001b[39m, \u001b[90m#unused#\u001b[39m::\u001b[0mVal\u001b[90m{6}\u001b[39m, \u001b[90mindex_tree\u001b[39m::\u001b[0mDynamicExpressions.EquationUtilsModule.NodeIndex, \u001b[90mcX\u001b[39m::\u001b[0mSubArray\u001b[90m{Float32, 2, Matrix{Float32}, Tuple{Base.Slice{Base.OneTo{Int64}}, Vector{Int64}}, false}\u001b[39m, \u001b[90mop\u001b[39m::\u001b[0mtypeof(+), \u001b[90mdiff_op\u001b[39m::\u001b[0mDynamicExpressions.DynamicExpressionsZygoteExt.var\"#3#4\"\u001b[90m{typeof(+)}\u001b[39m, \u001b[90moperators\u001b[39m::\u001b[0mDynamicExpressions.OperatorEnumModule.OperatorEnum, \u001b[90m#unused#\u001b[39m::\u001b[0mVal\u001b[90m{true}\u001b[39m, \u001b[90m#unused#\u001b[39m::\u001b[0mVal\u001b[90m{false}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[36mDynamicExpressions.EvaluateEquationDerivativeModule\u001b[39m \u001b[90m~/.julia/packages/DynamicExpressions/KRT17/src/\u001b[39m\u001b[90m\u001b[4mEvaluateEquationDerivative.jl:358\u001b[24m\u001b[39m\n         [13] \u001b[0m\u001b[1m_eval_grad_tree_array\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mtree\u001b[39m::\u001b[0mDynamicExpressions.EquationModule.Node\u001b[90m{Float32}\u001b[39m, \u001b[90m#unused#\u001b[39m::\u001b[0mVal\u001b[90m{6}\u001b[39m, \u001b[90mindex_tree\u001b[39m::\u001b[0mDynamicExpressions.EquationUtilsModule.NodeIndex, \u001b[90mcX\u001b[39m::\u001b[0mSubArray\u001b[90m{Float32, 2, Matrix{Float32}, Tuple{Base.Slice{Base.OneTo{Int64}}, Vector{Int64}}, false}\u001b[39m, \u001b[90moperators\u001b[39m::\u001b[0mDynamicExpressions.OperatorEnumModule.OperatorEnum, \u001b[90m#unused#\u001b[39m::\u001b[0mVal\u001b[90m{true}\u001b[39m, \u001b[90m#unused#\u001b[39m::\u001b[0mVal\u001b[90m{false}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[36mDynamicExpressions.EvaluateEquationDerivativeModule\u001b[39m \u001b[90m~/.julia/packages/DynamicExpressions/KRT17/src/\u001b[39m\u001b[90m\u001b[4mEvaluateEquationDerivative.jl:276\u001b[24m\u001b[39m\n         [14] \u001b[0m\u001b[1meval_grad_tree_array\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mtree\u001b[39m::\u001b[0mDynamicExpressions.EquationModule.Node\u001b[90m{Float32}\u001b[39m, \u001b[90m#unused#\u001b[39m::\u001b[0mVal\u001b[90m{6}\u001b[39m, \u001b[90mindex_tree\u001b[39m::\u001b[0mDynamicExpressions.EquationUtilsModule.NodeIndex, \u001b[90mcX\u001b[39m::\u001b[0mSubArray\u001b[90m{Float32, 2, Matrix{Float32}, Tuple{Base.Slice{Base.OneTo{Int64}}, Vector{Int64}}, false}\u001b[39m, \u001b[90moperators\u001b[39m::\u001b[0mDynamicExpressions.OperatorEnumModule.OperatorEnum, \u001b[90m#unused#\u001b[39m::\u001b[0mVal\u001b[90m{true}\u001b[39m, \u001b[90m#unused#\u001b[39m::\u001b[0mVal\u001b[90m{false}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[36mDynamicExpressions.EvaluateEquationDerivativeModule\u001b[39m \u001b[90m~/.julia/packages/DynamicExpressions/KRT17/src/\u001b[39m\u001b[90m\u001b[4mEvaluateEquationDerivative.jl:226\u001b[24m\u001b[39m\n         [15] \u001b[0m\u001b[1meval_grad_tree_array\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mtree\u001b[39m::\u001b[0mDynamicExpressions.EquationModule.Node\u001b[90m{Float32}\u001b[39m, \u001b[90mcX\u001b[39m::\u001b[0mSubArray\u001b[90m{Float32, 2, Matrix{Float32}, Tuple{Base.Slice{Base.OneTo{Int64}}, Vector{Int64}}, false}\u001b[39m, \u001b[90moperators\u001b[39m::\u001b[0mDynamicExpressions.OperatorEnumModule.OperatorEnum; \u001b[90mvariable\u001b[39m::\u001b[0mBool, \u001b[90mturbo\u001b[39m::\u001b[0mBool\u001b[0m\u001b[1m)\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[36mDynamicExpressions.EvaluateEquationDerivativeModule\u001b[39m \u001b[90m~/.julia/packages/DynamicExpressions/KRT17/src/\u001b[39m\u001b[90m\u001b[4mEvaluateEquationDerivative.jl:204\u001b[24m\u001b[39m\n         [16] \u001b[0m\u001b[1m#eval_grad_tree_array#2\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/SymbolicRegression/fHd3u/src/\u001b[39m\u001b[90m\u001b[4mInterfaceDynamicExpressions.jl:112\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n         [17] \u001b[0m\u001b[1mderivative_loss\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mtree\u001b[39m::\u001b[0mDynamicExpressions.EquationModule.Node\u001b[90m{Float32}\u001b[39m, \u001b[90mdataset\u001b[39m::\u001b[0mDataset\u001b[90m{Float32, Float32, Matrix{Float32}, Vector{Float32}, Nothing, NamedTuple{(:∂y,), Tuple{Matrix{Float32}}}, Nothing, Nothing, Nothing, Nothing}\u001b[39m, \u001b[90moptions\u001b[39m::\u001b[0mOptions\u001b[90m{Int64, DynamicExpressions.OperatorEnumModule.OperatorEnum, false, Optim.Options{Float64, Nothing}, StatsBase.Weights{Float64, Float64, Vector{Float64}}}\u001b[39m, \u001b[90midx\u001b[39m::\u001b[0mVector\u001b[90m{Int64}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[32mMain\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mIn[5]:6\u001b[24m\u001b[39m\n         [18] \u001b[0m\u001b[1mevaluator\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mf\u001b[39m::\u001b[0mtypeof(derivative_loss), \u001b[90mtree\u001b[39m::\u001b[0mDynamicExpressions.EquationModule.Node\u001b[90m{Float32}\u001b[39m, \u001b[90mdataset\u001b[39m::\u001b[0mDataset\u001b[90m{Float32, Float32, Matrix{Float32}, Vector{Float32}, Nothing, NamedTuple{(:∂y,), Tuple{Matrix{Float32}}}, Nothing, Nothing, Nothing, Nothing}\u001b[39m, \u001b[90moptions\u001b[39m::\u001b[0mOptions\u001b[90m{Int64, DynamicExpressions.OperatorEnumModule.OperatorEnum, false, Optim.Options{Float64, Nothing}, StatsBase.Weights{Float64, Float64, Vector{Float64}}}\u001b[39m, \u001b[90midx\u001b[39m::\u001b[0mVector\u001b[90m{Int64}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[33mSymbolicRegression.LossFunctionsModule\u001b[39m \u001b[90m~/.julia/packages/SymbolicRegression/fHd3u/src/\u001b[39m\u001b[90m\u001b[4mLossFunctions.jl:80\u001b[24m\u001b[39m\n         [19] \u001b[0m\u001b[1meval_loss\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mtree\u001b[39m::\u001b[0mDynamicExpressions.EquationModule.Node\u001b[90m{Float32}\u001b[39m, \u001b[90mdataset\u001b[39m::\u001b[0mDataset\u001b[90m{Float32, Float32, Matrix{Float32}, Vector{Float32}, Nothing, NamedTuple{(:∂y,), Tuple{Matrix{Float32}}}, Nothing, Nothing, Nothing, Nothing}\u001b[39m, \u001b[90moptions\u001b[39m::\u001b[0mOptions\u001b[90m{Int64, DynamicExpressions.OperatorEnumModule.OperatorEnum, false, Optim.Options{Float64, Nothing}, StatsBase.Weights{Float64, Float64, Vector{Float64}}}\u001b[39m; \u001b[90mregularization\u001b[39m::\u001b[0mBool, \u001b[90midx\u001b[39m::\u001b[0mVector\u001b[90m{Int64}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[33mSymbolicRegression.LossFunctionsModule\u001b[39m \u001b[90m~/.julia/packages/SymbolicRegression/fHd3u/src/\u001b[39m\u001b[90m\u001b[4mLossFunctions.jl:105\u001b[24m\u001b[39m\n         [20] \u001b[0m\u001b[1m#eval_loss_batched#4\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/SymbolicRegression/fHd3u/src/\u001b[39m\u001b[90m\u001b[4mLossFunctions.jl:119\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n         [21] \u001b[0m\u001b[1m#score_func_batched#6\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/SymbolicRegression/fHd3u/src/\u001b[39m\u001b[90m\u001b[4mLossFunctions.jl:181\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n         [22] \u001b[0m\u001b[1ms_r_cycle\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mdataset\u001b[39m::\u001b[0mDataset\u001b[90m{Float32, Float32, Matrix{Float32}, Vector{Float32}, Nothing, NamedTuple{(:∂y,), Tuple{Matrix{Float32}}}, Nothing, Nothing, Nothing, Nothing}\u001b[39m, \u001b[90mpop\u001b[39m::\u001b[0mPopulation\u001b[90m{Float32, Float32}\u001b[39m, \u001b[90mncycles\u001b[39m::\u001b[0mInt64, \u001b[90mcurmaxsize\u001b[39m::\u001b[0mInt64, \u001b[90mrunning_search_statistics\u001b[39m::\u001b[0mSymbolicRegression.AdaptiveParsimonyModule.RunningSearchStatistics; \u001b[90mverbosity\u001b[39m::\u001b[0mInt64, \u001b[90moptions\u001b[39m::\u001b[0mOptions\u001b[90m{Int64, DynamicExpressions.OperatorEnumModule.OperatorEnum, false, Optim.Options{Float64, Nothing}, StatsBase.Weights{Float64, Float64, Vector{Float64}}}\u001b[39m, \u001b[90mrecord\u001b[39m::\u001b[0mDict\u001b[90m{String, Any}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[33mSymbolicRegression.SingleIterationModule\u001b[39m \u001b[90m~/.julia/packages/SymbolicRegression/fHd3u/src/\u001b[39m\u001b[90m\u001b[4mSingleIteration.jl:62\u001b[24m\u001b[39m\n         [23] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/SymbolicRegression/fHd3u/src/\u001b[39m\u001b[90m\u001b[4mSymbolicRegression.jl:955\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n         [24] \u001b[0m\u001b[1m(::SymbolicRegression.var\"#52#79\"{Options{Int64, DynamicExpressions.OperatorEnumModule.OperatorEnum, false, Optim.Options{Float64, Nothing}, StatsBase.Weights{Float64, Float64, Vector{Float64}}}, Int64, Population{Float32, Float32}, SymbolicRegression.AdaptiveParsimonyModule.RunningSearchStatistics, Int64, Dataset{Float32, Float32, Matrix{Float32}, Vector{Float32}, Nothing, NamedTuple{(:∂y,), Tuple{Matrix{Float32}}}, Nothing, Nothing, Nothing, Nothing}})\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[0m\u001b[1m)\u001b[22m\n        \u001b[90m    @ \u001b[39m\u001b[33mSymbolicRegression\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mthreadingconstructs.jl:258\u001b[24m\u001b[39m",
      "",
      "Stacktrace:",
      "  [1] wait",
      "    @ ./task.jl:345 [inlined]",
      "  [2] fetch",
      "    @ ./task.jl:360 [inlined]",
      "  [3] _equation_search(#unused#::Val{:multithreading}, #unused#::Val{1}, datasets::Vector{Dataset{Float32, Float32, Matrix{Float32}, Vector{Float32}, Nothing, NamedTuple{(:∂y,), Tuple{Matrix{Float32}}}, Nothing, Nothing, Nothing, Nothing}}, niterations::Int64, options::Options{Int64, DynamicExpressions.OperatorEnumModule.OperatorEnum, false, Optim.Options{Float64, Nothing}, StatsBase.Weights{Float64, Float64, Vector{Float64}}}, numprocs::Nothing, procs::Nothing, addprocs_function::Nothing, runtests::Bool, saved_state::Nothing, verbosity::Int64, progress::Bool, #unused#::Val{true})",
      "    @ SymbolicRegression ~/.julia/packages/SymbolicRegression/fHd3u/src/SymbolicRegression.jl:831",
      "  [4] equation_search(datasets::Vector{Dataset{Float32, Float32, Matrix{Float32}, Vector{Float32}, Nothing, NamedTuple{(:∂y,), Tuple{Matrix{Float32}}}, Nothing, Nothing, Nothing, Nothing}}; niterations::Int64, options::Options{Int64, DynamicExpressions.OperatorEnumModule.OperatorEnum, false, Optim.Options{Float64, Nothing}, StatsBase.Weights{Float64, Float64, Vector{Float64}}}, parallelism::Symbol, numprocs::Nothing, procs::Nothing, addprocs_function::Nothing, runtests::Bool, saved_state::Nothing, return_state::Bool, verbosity::Int64, progress::Nothing, v_dim_out::Val{1})",
      "    @ SymbolicRegression ~/.julia/packages/SymbolicRegression/fHd3u/src/SymbolicRegression.jl:514",
      "  [5] equation_search(X::Matrix{Float32}, y::Matrix{Float32}; niterations::Int64, weights::Nothing, extra::NamedTuple{(:∂y,), Tuple{Matrix{Float32}}}, options::Options{Int64, DynamicExpressions.OperatorEnumModule.OperatorEnum, false, Optim.Options{Float64, Nothing}, StatsBase.Weights{Float64, Float64, Vector{Float64}}}, variable_names::Vector{String}, display_variable_names::Vector{String}, y_variable_names::Nothing, parallelism::Symbol, numprocs::Nothing, procs::Nothing, addprocs_function::Nothing, runtests::Bool, saved_state::Nothing, return_state::Bool, loss_type::Type{Nothing}, verbosity::Int64, progress::Nothing, X_units::Nothing, y_units::Nothing, v_dim_out::Val{1}, multithreaded::Nothing, varMap::Nothing)",
      "    @ SymbolicRegression ~/.julia/packages/SymbolicRegression/fHd3u/src/SymbolicRegression.jl:392",
      "  [6] #equation_search#24",
      "    @ ~/.julia/packages/SymbolicRegression/fHd3u/src/SymbolicRegression.jl:421 [inlined]",
      "  [7] _update(m::SRRegressor{DynamicQuantities.SymbolicDimensions{DynamicQuantities.FixedRational{Int32, 25200}}, DataType, false}, verbosity::Int64, old_fitresult::Nothing, old_cache::Nothing, X::Matrix{Float32}, y::Vector{Float32}, w::NamedTuple{(:∂y,), Tuple{Matrix{Float32}}}, options::Options{Int64, DynamicExpressions.OperatorEnumModule.OperatorEnum, false, Optim.Options{Float64, Nothing}, StatsBase.Weights{Float64, Float64, Vector{Float64}}})",
      "    @ SymbolicRegression.MLJInterfaceModule ~/.julia/packages/SymbolicRegression/fHd3u/src/MLJInterface.jl:152",
      "  [8] update(m::SRRegressor{DynamicQuantities.SymbolicDimensions{DynamicQuantities.FixedRational{Int32, 25200}}, DataType, false}, verbosity::Int64, old_fitresult::Nothing, old_cache::Nothing, X::Matrix{Float32}, y::Vector{Float32}, w::NamedTuple{(:∂y,), Tuple{Matrix{Float32}}})",
      "    @ SymbolicRegression.MLJInterfaceModule ~/.julia/packages/SymbolicRegression/fHd3u/src/MLJInterface.jl:124",
      "  [9] fit(m::SRRegressor{DynamicQuantities.SymbolicDimensions{DynamicQuantities.FixedRational{Int32, 25200}}, DataType, false}, verbosity::Int64, X::Matrix{Float32}, y::Vector{Float32}, w::NamedTuple{(:∂y,), Tuple{Matrix{Float32}}})",
      "    @ SymbolicRegression.MLJInterfaceModule ~/.julia/packages/SymbolicRegression/fHd3u/src/MLJInterface.jl:118",
      " [10] fit_only!(mach::Machine{SRRegressor{DynamicQuantities.SymbolicDimensions{DynamicQuantities.FixedRational{Int32, 25200}}, DataType, false}, true}; rows::Nothing, verbosity::Int64, force::Bool, composite::Nothing)",
      "    @ MLJBase ~/.julia/packages/MLJBase/ByFwA/src/machines.jl:680",
      " [11] fit_only!",
      "    @ ~/.julia/packages/MLJBase/ByFwA/src/machines.jl:606 [inlined]",
      " [12] #fit!#63",
      "    @ ~/.julia/packages/MLJBase/ByFwA/src/machines.jl:778 [inlined]",
      " [13] fit!(mach::Machine{SRRegressor{DynamicQuantities.SymbolicDimensions{DynamicQuantities.FixedRational{Int32, 25200}}, DataType, false}, true})",
      "    @ MLJBase ~/.julia/packages/MLJBase/ByFwA/src/machines.jl:775",
      " [14] top-level scope",
      "    @ In[9]:17"
     ]
    }
   ],
   "source": [
    "model = SRRegressor(;\n",
    "    binary_operators=[+, *, ^],\n",
    "    unary_operators=[log],\n",
    "    constraints=[(^)=>(-1, 9)],\n",
    "    nested_constraints=[(^) => [(^) => 0, log => 0],\n",
    "                   log => [(^) =>  0, log => 0],\n",
    "            #        exp => [log => 0]\n",
    "        ],\n",
    "    loss_function=derivative_loss,\n",
    "    enable_autodiff=true,\n",
    "    batching=false,\n",
    "    #batch_size=100,\n",
    "    niterations=100,\n",
    "    parsimony=100,\n",
    ")\n",
    "mach = machine(model, X, y, (; ∂y=∂y))\n",
    "fit!(mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf72f2c-3b4b-47a7-8980-0aee00aee91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = report(mach)\n",
    "eq = r.equations[r.best_idx]\n",
    "\n",
    "variable_names = [\"A_\", \"B_\", \"C_\", \"D_\", \"E_\", \"F_\"]\n",
    "symbolic_eq = node_to_symbolic(eq, model, variable_names=variable_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59adb52e-aa79-4bd0-9925-dcce90f31615",
   "metadata": {},
   "source": [
    "## loss for fitting $y=\\partial y$\n",
    "here we want to pass the derivatives as an extra and fit the candidate expression to that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ff80e74-eb92-43f4-81cf-6474d9d5c783",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "old_derivative_loss (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function old_derivative_loss(tree, dataset::Dataset{T,L}, options, idx) where {T,L}\n",
    "    # Column-major:\n",
    "    X = idx === nothing ? dataset.X : view(dataset.X, :, idx)\n",
    "    ∂y = idx === nothing ? dataset.y : view(dataset.y, idx)\n",
    "\n",
    "    ŷ, ∂ŷ, completed = eval_grad_tree_array(tree, X, options; variable=true)\n",
    "\n",
    "    !completed && return L(Inf)\n",
    "\n",
    "    mse = sum(i -> (∂ŷ[i] - ∂y[i])^2, eachindex(∂y)) / length(∂y)\n",
    "    return mse\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89ea2000-5252-487d-a2be-e4982d8e9533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fit_derivative_loss (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function fit_derivative_loss(tree, dataset::Dataset{T,L}, options, idx) where {T,L}\n",
    "    # Column-major:\n",
    "    X = idx === nothing ? dataset.X : view(dataset.X, :, idx)\n",
    "    #∂y = idx === nothing ? dataset.y : view(dataset.y, idx)\n",
    "\n",
    "    ŷ, ∂ŷ, completed = eval_grad_tree_array(tree, X, options; variable=true)\n",
    "\n",
    "    !completed && return L(Inf)\n",
    "\n",
    "    y = idx === nothing ? dataset.y : view(dataset.y, idx)\n",
    "    ∂y = idx === nothing ? dataset.extra.∂y : view(dataset.extra.∂y, idx)\n",
    "    \n",
    "    # match the derivative only\n",
    "    mse = sum(i -> (∂ŷ[i] - ∂y[i])^2, eachindex(∂y)) / length(∂y)\n",
    "    \n",
    "    return mse\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0778bec2-992d-4e1c-b122-ba2afd6684a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 6), (1000,), (1000, 6))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in numpy data\n",
    "\n",
    "skip = 10\n",
    "\n",
    "jacobian = npzread(\"mean_jacobian_for_sr.npy\")\n",
    "\n",
    "X = npzread(\"theta_eta_for_sr_model_0.npy\")[1:skip:10000, 1:6]\n",
    "\n",
    "y = npzread(\"theta_eta_for_sr_model_0.npy\")[1:skip:10000, 7]\n",
    "\n",
    "\n",
    "∂y = jacobian[1:skip:10000, 1, :] # learn the first element of the jacobian row \n",
    "# this will be the integral wrt A\n",
    "\n",
    "size(X), size(y), size(∂y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5d8049-cd80-4cba-ae7f-193cc1676ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SRRegressor(;\n",
    "    binary_operators=[+, *, ^],\n",
    "    unary_operators=[log],\n",
    "    constraints=[(^)=>(-1, 9)],\n",
    "    nested_constraints=[(^) => [(^) => 0, log => 0],\n",
    "                   log => [(^) =>  0, log => 0],\n",
    "            #        exp => [log => 0]\n",
    "        ],\n",
    "    loss_function=fit_derivative_loss,\n",
    "    enable_autodiff=true,\n",
    "    batching=true,\n",
    "    batch_size=100,\n",
    "    niterations=100,\n",
    "    parsimony=100,\n",
    ")\n",
    "mach = machine(model, X, y, (; ∂y=∂y))\n",
    "fit!(mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40e26ad-c725-4a23-9f24-8f28ab7fa4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = report(mach)\n",
    "eq = r.equations[r.best_idx]\n",
    "\n",
    "variable_names = [\"mu\", \"Sigma\"]\n",
    "symbolic_eq = node_to_symbolic(eq, model, variable_names=variable_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
